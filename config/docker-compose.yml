version: '3.8'

services:
  whatsapp-rag:
    build: 
      context: ..
      dockerfile: config/Dockerfile
    container_name: whatsapp-rag
    ports:
      - "${PORT:-7860}:7860"
    environment:
      # Core configuration
      - HOST=0.0.0.0
      - PORT=7860
      
      # Pass through all environment variables from .env file
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GH_MODELS_BASE_URL=${GH_MODELS_BASE_URL:-https://models.github.ai/inference}
      - CHAT_MODEL=${CHAT_MODEL:-openai/gpt-4o}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-openai/text-embedding-3-small}
      - USE_LOCAL_EMBEDDINGS=${USE_LOCAL_EMBEDDINGS:-0}
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-intfloat/multilingual-e5-small}
      
      # Performance settings
      - EMBED_BATCH_SIZE=${EMBED_BATCH_SIZE:-64}
      - EMBED_MAX_CHARS_PER_REQUEST=${EMBED_MAX_CHARS_PER_REQUEST:-60000}
      - EMBED_MAX_CHARS_PER_ITEM=${EMBED_MAX_CHARS_PER_ITEM:-4000}
      
      # Gradio settings
      - GRADIO_SHARE=${GRADIO_SHARE:-0}
      - GRADIO_ANALYTICS_ENABLED=${GRADIO_ANALYTICS_ENABLED:-0}
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
    volumes:
      # Persist uploaded data
      - ../data:/app/data
      
      # Optional: mount custom models cache to speed up restarts
      - huggingface_cache:/root/.cache/huggingface
      
      # Optional: mount logs for external monitoring
      - ../logs:/app/logs
      
    restart: unless-stopped
    
    # Resource limits (adjust based on your system)
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  huggingface_cache:
    driver: local
    
# Optional: Add nginx reverse proxy
  nginx:
    image: nginx:alpine
    container_name: whatsapp-rag-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - whatsapp-rag
    restart: unless-stopped
    profiles:
      - with-proxy

# Optional: Add monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: whatsapp-rag-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    restart: unless-stopped
    profiles:
      - monitoring
      
  grafana:
    image: grafana/grafana:latest
    container_name: whatsapp-rag-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  grafana_data:
    driver: local