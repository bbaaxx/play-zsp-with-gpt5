"""
Analysis and agent-based trend detection functionality.
"""

from __future__ import annotations

import os
import logging
from typing import Optional
from dataclasses import dataclass

from rag import ChatDataFrame, ChatAnalyzer, AnalysisResult, AdaptiveAnalyzer, AdaptiveAnalysisResult
from rag.llm_providers import LLMManager

logger = logging.getLogger(__name__)


@dataclass
class AnalysisConfig:
    """Configuration for analysis operations."""
    github_token: Optional[str] = None
    model_name: Optional[str] = None
    
    def __post_init__(self):
        if self.github_token is None:
            self.github_token = os.environ.get("GITHUB_TOKEN")
        if self.model_name is None:
            self.model_name = os.environ.get("CHAT_MODEL", "gpt-4o-mini")


class AnalyticsEngine:
    """Handles analysis and agent-based trend detection functionality."""
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        """Initialize analytics engine with configuration."""
        self.config = config or AnalysisConfig()
        self.last_analysis: Optional[AnalysisResult] = None
        self.last_adaptive_analysis: Optional[AdaptiveAnalysisResult] = None
        self.last_analysis_summary: Optional[str] = None
        self.last_adaptive_summary: Optional[str] = None
        self.llm_manager = LLMManager()
        self._chat_processor: Optional[object] = None
    
    def _validate_configuration(self) -> Optional[str]:
        """Validate that required configuration is present."""
        if not self.config.github_token:
            return (
                "‚ùå **Error de configuraci√≥n**\n\n"
                "Para usar el an√°lisis inteligente, necesitas configurar tu token de GitHub:\n\n"
                "1. Crea un archivo `.env` en la ra√≠z del proyecto\n"
                "2. Agrega la l√≠nea: `GITHUB_TOKEN=tu_token_aqu√≠`\n"
                "3. Obt√©n tu token en: https://github.com/settings/tokens\n\n"
                "El token necesita acceso a GitHub Models para funcionar."
            )
        return None
    
    def _format_basic_analysis_results(self, result: AnalysisResult) -> str:
        """Format basic analysis results for display."""
        output = ["=== AN√ÅLISIS INTELIGENTE DE CONVERSACI√ìN ===\n"]
        
        # Tendencias identificadas
        if result.trend_summaries:
            output.append("üîç **TENDENCIAS Y PATRONES:**")
            for i, trend in enumerate(result.trend_summaries, 1):
                output.append(f"\n{i}. **{trend.trend_type.upper()}** (confianza: {trend.confidence_score:.1%})")
                output.append(f"   {trend.description}")
                if trend.time_period:
                    output.append(f"   Per√≠odo: {trend.time_period}")
                if trend.participants:
                    output.append(f"   Participantes: {', '.join(trend.participants)}")
        else:
            output.append("üîç **TENDENCIAS Y PATRONES:** No se detectaron patrones significativos.")
        
        # Anomal√≠as detectadas
        output.append("\n‚ö†Ô∏è **COMPORTAMIENTOS INUSUALES:**")
        if result.anomalies:
            for i, anomaly in enumerate(result.anomalies, 1):
                severity_icon = "üü•" if anomaly.severity == "high" else "üü®" if anomaly.severity == "medium" else "üü©"
                output.append(f"\n{i}. {severity_icon} **{anomaly.anomaly_type.upper()}**")
                output.append(f"   {anomaly.description}")
                if anomaly.participant:
                    output.append(f"   Participante: {anomaly.participant}")
        else:
            output.append("No se detectaron comportamientos inusuales.")
        
        # Mensajes memorables
        output.append("\nüí¨ **MENSAJES MEMORABLES:**")
        if result.quotable_messages:
            for i, quote in enumerate(result.quotable_messages, 1):
                type_icon = "üòÇ" if quote.quote_type == "funny" else "üí≠" if quote.quote_type == "insightful" else "‚ù§Ô∏è" if quote.quote_type == "emotional" else "‚≠ê"
                output.append(f"\n{i}. {type_icon} **{quote.sender}** ({quote.timestamp.strftime('%Y-%m-%d %H:%M')})")
                output.append(f"   \"{quote.message}\"")
                output.append(f"   Relevancia: {quote.relevance_score:.1%} | Tipo: {quote.quote_type}")
                if quote.context:
                    output.append(f"   Contexto: {quote.context}")
        else:
            output.append("No se encontraron mensajes especialmente memorables.")
        
        # Metadatos
        output.append("\nüìä **METADATOS DEL AN√ÅLISIS:**")
        output.append(f"- Mensajes analizados: {result.analysis_metadata.get('total_messages_analyzed', 0)}")
        output.append(f"- Modelo usado: {result.analysis_metadata.get('model_used', 'N/A')}")
        output.append(f"- An√°lisis realizado: {result.analysis_metadata.get('analysis_timestamp', 'N/A')}")
        
        return "\n".join(output)
    
    def _format_adaptive_analysis_results(self, result: AdaptiveAnalysisResult) -> str:
        """Format adaptive analysis results for display."""
        output = ["=== AN√ÅLISIS ADAPTATIVO DE DOS ETAPAS ===\n"]
        
        # Contextos detectados
        output.append("üéØ **CONTEXTOS DETECTADOS:**")
        if result.detected_contexts:
            for i, context in enumerate(result.detected_contexts, 1):
                confidence_text = "üü¢ Alta" if context.confidence > 0.7 else "üü° Media" if context.confidence > 0.4 else "üî¥ Baja"
                output.append(f"\n{i}. **{context.category.replace('_', ' ').title()}** - {confidence_text} ({context.confidence:.1%})")
                if context.evidence:
                    output.append("   üìã Evidencia:")
                    for evidence_item in context.evidence:
                        output.append(f"     ‚Ä¢ {evidence_item}")
        else:
            output.append("No se detectaron contextos espec√≠ficos.")
        
        # An√°lisis especializados
        output.append("\nüî¨ **AN√ÅLISIS ESPECIALIZADOS:**")
        if result.specialized_analyses:
            for context_type, analyses in result.specialized_analyses.items():
                context_name = context_type.replace('_', ' ').title()
                output.append(f"\nüìä **{context_name}:**")
                
                if isinstance(analyses, dict) and "error" not in analyses:
                    for focus_area, analysis in analyses.items():
                        if analysis and isinstance(analysis, str):
                            area_name = focus_area.replace('_', ' ').title()
                            # Show complete analysis without truncation
                            output.append(f"   ‚Ä¢ **{area_name}**:")
                            # Format the analysis with proper indentation
                            analysis_lines = analysis.strip().split('\n')
                            for line in analysis_lines:
                                if line.strip():
                                    output.append(f"     {line}")
                            output.append("")  # Add blank line for readability
                elif "error" in analyses:
                    output.append(f"   ‚ö†Ô∏è Error en an√°lisis: {analyses['error']}")
        else:
            output.append("No se generaron an√°lisis especializados.")
        
        # Insights adaptativos
        output.append("\nüí° **INSIGHTS ADAPTATIVOS:**")
        if result.adaptive_insights:
            for insight in result.adaptive_insights:
                output.append(f"\n{insight}")
        else:
            output.append("No se generaron insights adaptativos.")
        
        # An√°lisis b√°sico (resumen)
        basic = result.basic_analysis
        output.append("\nüìà **RESUMEN AN√ÅLISIS B√ÅSICO:**")
        output.append(f"‚Ä¢ {len(basic.trend_summaries)} tendencias identificadas")
        output.append(f"‚Ä¢ {len(basic.anomalies)} anomal√≠as detectadas")
        output.append(f"‚Ä¢ {len(basic.quotable_messages)} mensajes memorables")
        
        # Metadatos
        output.append("\nüìä **METADATOS:**")
        output.append(f"- Contextos detectados: {result.analysis_metadata.get('total_contexts_detected', 0)}")
        output.append(f"- Agentes especializados: {result.analysis_metadata.get('specialized_agents_used', 0)}")
        output.append(f"- Versi√≥n an√°lisis: {result.analysis_metadata.get('analysis_version', 'N/A')}")
        output.append(f"- Realizado: {result.analysis_metadata.get('analysis_timestamp', 'N/A')[:19]}")
        
        return "\n".join(output)
    
    def _generate_analysis_summary(self, analysis_text: str, analysis_type: str = "an√°lisis") -> str:
        """Generate an LLM-powered summary of analysis results."""
        try:
            messages = [
                {
                    "role": "system", 
                    "content": f"""Eres un experto en an√°lisis de datos conversacionales. Tu tarea es generar un resumen ejecutivo conciso y de alto nivel de los resultados del {analysis_type} de WhatsApp proporcionado.

El resumen debe:
1. Identificar los 3-4 hallazgos m√°s importantes y relevantes
2. Presentar insights clave de manera clara y procesable
3. Usar un tono profesional pero accesible
4. Mantenerse entre 200-300 palabras
5. Usar bullet points o estructura clara
6. Enfocarse en patrones, tendencias y anomal√≠as significativas

Responde SOLO con el resumen, sin pre√°mbulos ni explicaciones adicionales."""
                },
                {
                    "role": "user", 
                    "content": f"Por favor, genera un resumen ejecutivo de este {analysis_type}:\n\n{analysis_text}"
                }
            ]
            
            summary = self.llm_manager.generate_response(
                messages=messages,
                temperature=0.3,
                max_tokens=400
            )
            
            return summary.strip()
            
        except Exception as e:
            logger.exception("Error generando resumen del an√°lisis")
            return f"‚ö†Ô∏è Error generando resumen: {e}"
    
    def analyze_chat_basic(self, chat_dataframe: ChatDataFrame, progress_callback=None) -> str:
        """Perform basic intelligent chat analysis using smolagents."""
        if progress_callback is not None:
            progress_callback(0, desc="Iniciando an√°lisis...")
        
        if chat_dataframe is None or chat_dataframe.is_empty:
            return "No hay datos de chat cargados para analizar. Primero indexa un archivo."
        
        if progress_callback is not None:
            progress_callback(0.1, desc="Verificando configuraci√≥n...")
        
        # Check configuration
        config_error = self._validate_configuration()
        if config_error:
            return config_error
        
        try:
            if progress_callback is not None:
                progress_callback(0.2, desc="Configurando analizador...")
            
            # Create analyzer
            analyzer = ChatAnalyzer(llm_model_name=self.config.model_name)
            
            if progress_callback is not None:
                progress_callback(0.3, desc="Ejecutando an√°lisis inteligente... (esto puede tomar algunos minutos)")
            
            # Perform full analysis
            result = analyzer.full_analysis(chat_dataframe)
            self.last_analysis = result
            
            if progress_callback is not None:
                progress_callback(0.8, desc="Procesando resultados...")
            
            formatted_results = self._format_basic_analysis_results(result)
            
            if progress_callback is not None:
                progress_callback(0.9, desc="Generando resumen ejecutivo...")
            
            # Generate LLM-powered summary
            self.last_analysis_summary = self._generate_analysis_summary(formatted_results, "an√°lisis b√°sico")
            
            # Store analysis in vector database and update system prompt
            self._integrate_analysis_results(formatted_results, self.last_analysis_summary, "basic_analysis")
            
            if progress_callback is not None:
                progress_callback(1.0, desc="‚úÖ An√°lisis completado")
            
            return formatted_results
            
        except Exception as e:
            logger.exception("Error durante el an√°lisis inteligente")
            return f"Error durante el an√°lisis: {e}\n\nVerifica que tengas configurado correctamente el acceso a modelos LLM (variables de entorno GITHUB_TOKEN, etc.)"
    
    def analyze_chat_adaptive(self, chat_dataframe: ChatDataFrame, progress_callback=None) -> str:
        """Perform adaptive two-stage analysis."""
        if progress_callback is not None:
            progress_callback(0, desc="Iniciando an√°lisis adaptativo...")
        
        if chat_dataframe is None or chat_dataframe.is_empty:
            return "No hay datos de chat cargados para analizar. Primero indexa un archivo."
        
        if progress_callback is not None:
            progress_callback(0.1, desc="Verificando configuraci√≥n...")
        
        # Check configuration
        config_error = self._validate_configuration()
        if config_error:
            return config_error.replace("an√°lisis inteligente", "an√°lisis adaptativo")
        
        try:
            if progress_callback is not None:
                progress_callback(0.2, desc="Configurando analizador adaptativo...")
            
            # Create adaptive analyzer
            adaptive_analyzer = AdaptiveAnalyzer()
            
            if progress_callback is not None:
                progress_callback(0.3, desc="Etapa 1: Ejecutando an√°lisis b√°sico...")
                progress_callback(0.5, desc="Etapa 2: Detectando contextos de conversaci√≥n...")
                progress_callback(0.7, desc="Etapa 3: Creando agentes especializados...")
                progress_callback(0.85, desc="Etapa 4: Ejecutando an√°lisis especializados...")
            
            # Perform full adaptive analysis
            result = adaptive_analyzer.analyze(chat_dataframe)
            self.last_adaptive_analysis = result
            
            if progress_callback is not None:
                progress_callback(0.92, desc="Formateando resultados...")
            
            formatted_results = self._format_adaptive_analysis_results(result)
            
            if progress_callback is not None:
                progress_callback(0.96, desc="Generando resumen ejecutivo...")
            
            # Generate LLM-powered summary
            self.last_adaptive_summary = self._generate_analysis_summary(formatted_results, "an√°lisis adaptativo")
            
            # Store analysis in vector database and update system prompt
            self._integrate_analysis_results(formatted_results, self.last_adaptive_summary, "adaptive_analysis")
            
            if progress_callback is not None:
                progress_callback(1.0, desc="‚úÖ An√°lisis adaptativo completado")
            
            return formatted_results
            
        except Exception as e:
            logger.exception("Error durante el an√°lisis adaptativo")
            return f"Error durante el an√°lisis adaptativo: {e}\n\nVerifica que tengas configurado correctamente el acceso a modelos LLM."
    
    def get_analysis_summary(self) -> str:
        """Get a quick summary of the last analyses."""
        if self.last_analysis is None and self.last_adaptive_analysis is None:
            return "No hay an√°lisis previo disponible."
        
        summary = ["üìä **√öltimos An√°lisis:**"]
        
        # Basic analysis summary
        if self.last_analysis is not None:
            result = self.last_analysis
            summary.extend([
                "\nüîç **An√°lisis B√°sico:**",
                f"‚Ä¢ {len(result.trend_summaries)} tendencias identificadas",
                f"‚Ä¢ {len(result.anomalies)} anomal√≠as detectadas", 
                f"‚Ä¢ {len(result.quotable_messages)} mensajes memorables",
                f"‚Ä¢ Realizado: {result.analysis_metadata.get('analysis_timestamp', 'N/A')[:19]}"
            ])
        
        # Adaptive analysis summary
        if self.last_adaptive_analysis is not None:
            result = self.last_adaptive_analysis
            summary.extend([
                "\nüéØ **An√°lisis Adaptativo:**",
                f"‚Ä¢ {len(result.detected_contexts)} contextos detectados",
                f"‚Ä¢ {result.analysis_metadata.get('specialized_agents_used', 0)} agentes especializados",
                f"‚Ä¢ {len(result.adaptive_insights)} insights adaptativos",
                f"‚Ä¢ Realizado: {result.analysis_metadata.get('analysis_timestamp', 'N/A')[:19]}"
            ])
        
        return "\n".join(summary)
    
    def get_last_basic_summary(self) -> Optional[str]:
        """Get the summary of the last basic analysis."""
        return self.last_analysis_summary
    
    def get_last_adaptive_summary(self) -> Optional[str]:
        """Get the summary of the last adaptive analysis."""
        return self.last_adaptive_summary
    
    def set_chat_processor(self, chat_processor: object):
        """Set the chat processor reference for integration."""
        self._chat_processor = chat_processor
        
    def _integrate_analysis_results(self, full_analysis: str, summary: str, analysis_type: str):
        """Integrate analysis results into the chat system."""
        if not self._chat_processor:
            logger.warning("No chat processor available for analysis integration")
            return
            
        try:
            # Store full analysis in vector database
            self._chat_processor.add_analysis_to_vector_store(full_analysis, analysis_type)
            
            # Update system prompt with analysis summary
            self._chat_processor.update_system_prompt_with_analysis(summary)
            
            logger.info("Successfully integrated %s results into chat system", analysis_type)
        except Exception as e:
            logger.exception("Error integrating analysis results: %s", e)
        
    def clear_analysis_state(self):
        """Clear analysis state."""
        self.last_analysis = None
        self.last_adaptive_analysis = None
        self.last_analysis_summary = None
        self.last_adaptive_summary = None
        logger.info("Estado de an√°lisis limpiado")